{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_init():\n",
    "    with open('log.txt', 'w') as log:\n",
    "            log.write('Beginning Log\\n')\n",
    "def log_write(text):\n",
    "    with open('log.txt', 'a') as log:\n",
    "            log.write(f'{text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ps4.csv', 'r') as f:\n",
    "    read = csv.DictReader(f)\n",
    "    ps4 = [dict(row) for row in read]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review scraping functions\n",
    "#### things to add:\n",
    "* include url in each review dict, (and maybe status code)\n",
    "* handle 429 better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews_by_console(console, filename):\n",
    "    start = time.time()\n",
    "    log_init()\n",
    "    with open(filename, 'w') as f:\n",
    "        reviews = []\n",
    "        i = 0\n",
    "        for game in console:\n",
    "            log_write(f\"Starting scraping reviews for {game['title']}\")\n",
    "            game_reviews = extract_reviews_from_game(game)\n",
    "            [review.update({'GID': game['GID']}) for review in game_reviews]\n",
    "            reviews.append(game_reviews)\n",
    "            if i ==0:\n",
    "                fields=game_reviews[0].keys()\n",
    "                dw = csv.DictWriter(f, fieldnames=fields)\n",
    "                dw.writeheader()\n",
    "            dw.writerows(game_reviews)\n",
    "            i +=1\n",
    "            log_write(f\"Finished Scraping reviews for {game['title']}\")\n",
    "            log_write(f\"Scraped {game['GID']}/{len(console)}\")\n",
    "            log_write(f\"Elapsed Time: {time.time() - start}\")\n",
    "        log_write(f\"Scraped all reviews in {time.time() - start} seconds\")\n",
    "        return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews_from_game(game, agent= 0):\n",
    "    review_list = []\n",
    "    #generate url from game object\n",
    "    url = \"http://www.metacritic.com\" + game['page'] + \"/user-reviews\"\n",
    "    # get page request and soup object\n",
    "    headers = {'User-agent': f'1.{game[\"title\"]}'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    if(res.status_code > 300):\n",
    "        log_write(f\"Could not reach game: {url}, status code: {res.status_code}\")\n",
    "        if agent > 3:\n",
    "            return review_list\n",
    "        else:\n",
    "            log_write(f\"Trying again: {agent}\")\n",
    "            time.sleep(5)\n",
    "            extract_reviews_from_page(url, game, agent+1)\n",
    "    \n",
    "    page_source = res.content\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    try:\n",
    "        num_pages = soup.find('li', attrs={'class': 'last_page'}).find('a').text\n",
    "    except:\n",
    "        num_pages = 1\n",
    "    for i in range(int(num_pages)):\n",
    "        time.sleep(1)\n",
    "        new_url = f\"{url}?page={i}\"\n",
    "        revs = extract_reviews_from_page(new_url, game)\n",
    "        review_list.extend(revs)\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews_from_page(url, game, agent = 0):\n",
    "    \n",
    "    review_list = []\n",
    "    \n",
    "    # get page request and soup object\n",
    "    headers = {'User-agent': f\"{game['title']} . {url[-1]} . {agent}\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    log_write(f\"Accessing page at {url}\")\n",
    "    if(res.status_code > 300):\n",
    "        log_write(f\"Could not reach page: {url}, status code: {res.status_code}\")\n",
    "        if agent > 3:\n",
    "            return review_list\n",
    "        else:\n",
    "            time.sleep(5)\n",
    "            extract_reviews_from_page(url, game, agent+1)\n",
    "    \n",
    "    page_source = res.content\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    #save a list of reviews\n",
    "    try:\n",
    "        reviews = soup.find('ol', attrs={'class' : 'user_reviews'}).find_all('div', attrs={'class': 'review_content'})\n",
    "    except:\n",
    "        log_write(f\"No reviews found on page: {url}\")\n",
    "        return review_list\n",
    "    # loop through all reviews:\n",
    "    for review in reviews:\n",
    "        # extract the text, the review score, and the author, and date\n",
    "        try:\n",
    "            review_list.append(extract_single_review(review))\n",
    "        except: \n",
    "            print(review)\n",
    "    return review_list\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_review(soup):\n",
    "    review = {}\n",
    "    review['author'] = soup.find('a').text\n",
    "    review['date'] = soup.find('div', attrs={'class':'date'}).text\n",
    "    review['score'] = soup.find('div', attrs={'class': 'metascore_w'}).text\n",
    "    review['blurb'] = extract_text(soup.find('div', attrs={'class': 'review_body'}))\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_text(review):\n",
    "    if(not review.find('span', attrs={'class': 'blurb_expanded'})):\n",
    "        return review.find('span').text\n",
    "    else:\n",
    "        return review.find('span', attrs={'class': 'blurb_expanded'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews = extract_reviews_by_console(ps4, 'reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-agent': 'alsdkjflkjasdflk'}\n",
    "res = requests.get('http://www.metacritic.com/game/playstation-4/the-witcher-3-wild-hunt/user-reviews?page=5', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/conda/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
